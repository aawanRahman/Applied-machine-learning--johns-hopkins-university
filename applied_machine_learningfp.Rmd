---
title : "PRACTICAL MACHINE LEARNING PREDICTION ASSIGNMENT PROJECT-john hopkins university"


---
*auther*: awan-ur-rahman

*date*  : 29/11/18



PROJECT OVERVIEW :
--------------------
--------------------
#Background :
This is a PEER-graded assignment provided by JOHNS HOPKINS UNIVERSITY in coursera.The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 

for more information :
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

#Data : 

The training data for this project are available here :

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv




#Environments setup:

we need to load the packages for the upcoming processing .

<!-- checks for the woking directory . -->
check the current working directory.

    getwd()
clear gloabal environment variable memory list.

    rm(list = ls())
    
now migrate to the working directory.

    setwd("F:/tutorials/coursera/practical machine learning-john_hopkins university/week 4/coursera project")
**now load the packages needed for futher processing .**

    library( knitr)

    library(rattle)  <!-- provides graphlical user interface -->

    library(caret) <!-- this load the ggplot2 & lattice packages -->

    library(randomForest)

    library(rpart)

    library(rpart.plot)

    library(corrplot)

    library(RColorBrewer)

**now its time to load the seed.**

    set.seed(12345)

#Data Loading :

Now by executing the following lines of codes data will be loaded for this project.

Data can be loaded directly from the course given url.Here is the necessary code for this given below :

    training_data <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))

    test_data <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

#Data Partioning :

we divide the training_data in two portion . One for training the model and other for validation. 70% of the training_data goes to traindata which will be used for training the model and rest 30% of the training_data goes to validationdata which will be used for checking the efficiency of the model.

    traindata <- training_data[createDataPartition(training_data$classe, p=0.7, list=FALSE), ]

    validationdata <-  training_data[- createDataPartition(training_data$classe, p=0.7, list=FALSE),]

    dim(traindata)

    dim(validationdata)

#Data Cleaning :

both dataset have 160 variable many of which contain NA  and nearly zero variance , so we have clean up the variables.


**cleaning up nearly zero variance. **

   
   
    validationdata  <- validationdata[, - (nearZeroVar(traindata))  ]
 

    traindata <- traindata[, - (nearZeroVar(traindata)) ]
       

    dim(traindata)


    dim(validationdata)

**remove that are mostly NA. **

 
    
     validationdata  <- validationdata[,  ( sapply( traindata, function(x) mean(is.na(x))) > 0.95) == FALSE]
     
    traindata <- traindata[, (sapply( traindata, function(x) mean(is.na(x))) > 0.95) == FALSE]
    
    dim(traindata)

    dim(validationdata)

**remove column that doesnot contribute .**

    traindata <- traindata[, -(1:5)]

    validationdata  <- validationdata[, -(1:5)]

    dim(traindata)

    dim(validationdata)

now by the above process the number of variable has reduced to 54 of both traindata and validation data.


#Prediction Model Outcomes :

**model : DECISION TREE**

    set.seed(12345)

now lets feed the dataset to the decision tree model.

    model <- rpart( classe ~ ., data = traindata, method="class")

now plot the rpart model.

    prp(model)
    
now estimate the performance of the model with checking the validationdata dataset.

    prediction <- predict(model, validationdata, type = "class")

now lets draw the confusion matrix for the validationdata dataset.

    confusionMatrix(validationdata$classe, prediction)
    
Here the accuracy of the decision tree model is 75.97%(in percentage.)

**model :RANDOM FOREST **

again set the seed.

    set.seed(12345)
    
now lets feed the dataset to the random forest model.
    
    controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
    modelRandForest <- train(classe ~ ., data=traindata, method="rf",trControl=controlRF)
    modelRandForest$finalModel
    
now estimate the performance of the model with checking the validationdata dataset.

    predictionRandF <- predict(modelRandForest, validationdata)
    
now lets draw the confusion matrix for the validationdata dataset.

    confusionMatrix(validationdata$classe, predictionRandF)

The accuracy of random forest on the validationdata dataset is 99.95%(in percentage.)

#Model Comparison :

From the above we get the accuracy on validationdata dataset of :
    
    RANDOM FOREST : 99.95%
    DECISION TREE : 75.97%
    So, 
    RANDOM FOREST > DECISION TREE
    
    
So it is clear that random forest algorithm provide better performance then decision tree according to above in this assignment.


#Quiz Assignment Answer:

so now i use random forest tree to generate the quiz assignment answer.


    pml_write_files = function(x){
      n = length(x)
      for(i in 1:n){
        filename = paste0("F:/tutorials/coursera/practical machine learning-john_hopkins university/week 4/coursera project/answer/problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
      }
    }
    pml_write_files(predict(modelRandForest, test_data[, -length(names(test_data))]))






























































































    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    



